{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# üìå Introduzione  \n\nIn questo notebook esploreremo come **ospitare ed eseguire modelli AI open-source con Ollama su Kaggle**, seguendo la guida proposta nell‚Äôarticolo:  \n\nüîó [*Unleash the Power of AI: Host Your Own Ollama Models for Free with Google Colab*](https://medium.com/data-science-collective/unleash-the-power-of-ai-host-your-own-ollama-models-for-free-with-google-colab-0aac5f237a9f)  \n\n## üéØ Obiettivo  \nL'obiettivo √® testare la capacit√† di eseguire modelli **LLM (Large Language Models)** direttamente su **Kaggle**, sfruttando **Ollama**, una piattaforma progettata per semplificare il deployment di modelli AI su infrastrutture locali o cloud.  \n\n## üõ†Ô∏è Cosa faremo?  \n‚úÖ Configureremo **Kaggle** per supportare **Ollama**  \n‚úÖ Scaricheremo e avvieremo **un modello AI open-source**  \n‚úÖ Effettueremo test di inferenza per valutarne le prestazioni  \n\nQuesta soluzione consente di **eseguire modelli AI gratuitamente** senza bisogno di infrastrutture complesse, rappresentando un'opzione interessante per sviluppatori e ricercatori che vogliono sperimentare con LLM senza costi aggiuntivi.  \n\n## üìå Prerequisiti  \nüîπ Conoscenza di base di **Python** e dei **modelli AI** (opzionale, ma consigliata)  \n\nüì¢ **Nota:**  \nPoich√© Kaggle ha alcune **limitazioni di tempo e risorse hardware**, valuteremo anche eventuali **ottimizzazioni** per migliorare l'esperienza d‚Äôuso.  \n","metadata":{}},{"cell_type":"code","source":"!nproc","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!apt-get install lshw\n\n# Aggiornamento sistema e installazione CUDA\n!apt-get install -y --no-install-recommends \\\n    cuda-toolkit-12-2 \\\n    libnccl2 \\\n    libnccl-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:11:02.272952Z","iopub.execute_input":"2025-04-02T14:11:02.273319Z","iopub.status.idle":"2025-04-02T14:11:07.952932Z","shell.execute_reply.started":"2025-04-02T14:11:02.273280Z","shell.execute_reply":"2025-04-02T14:11:07.951900Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nlshw is already the newest version (02.19.git.2021.06.19.996aaad9c7-2build1).\n0 upgraded, 0 newly installed, 0 to remove and 129 not upgraded.\nReading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\nThe following additional packages will be installed:\n  cuda-documentation-12-2 cuda-nsight-12-2 cuda-nsight-systems-12-2 cuda-nvvp-12-2 cuda-tools-12-2\n  cuda-visual-tools-12-2 default-jre default-jre-headless gds-tools-12-2 libtinfo5 libxcb-icccm4\n  libxcb-image0 libxcb-keysyms1 libxcb-render-util0 libxcb-util1 libxcb-xinerama0 libxcb-xinput0\n  libxcb-xkb1 libxkbcommon-x11-0 libxtst6 nsight-systems-2023.2.3 openjdk-11-jdk-headless\n  openjdk-11-jre openjdk-11-jre-headless\nSuggested packages:\n  openjdk-11-demo openjdk-11-source libnss-mdns fonts-dejavu-extra fonts-ipafont-gothic\n  fonts-ipafont-mincho fonts-wqy-microhei | fonts-wqy-zenhei fonts-indic\nRecommended packages:\n  libatk-wrapper-java-jni fonts-dejavu-extra\nThe following NEW packages will be installed:\n  cuda-documentation-12-2 cuda-nsight-12-2 cuda-nsight-systems-12-2 cuda-nvvp-12-2\n  cuda-toolkit-12-2 cuda-tools-12-2 cuda-visual-tools-12-2 default-jre default-jre-headless\n  gds-tools-12-2 libtinfo5 libxcb-icccm4 libxcb-image0 libxcb-keysyms1 libxcb-render-util0\n  libxcb-util1 libxcb-xinerama0 libxcb-xinput0 libxcb-xkb1 libxkbcommon-x11-0 libxtst6\n  nsight-systems-2023.2.3 openjdk-11-jre\nThe following held packages will be changed:\n  libnccl-dev libnccl2\nThe following packages will be upgraded:\n  libnccl-dev libnccl2 openjdk-11-jdk-headless openjdk-11-jre-headless\n4 upgraded, 23 newly installed, 0 to remove and 125 not upgraded.\nE: Held packages were changed and -y was used without --allow-change-held-packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Impostazione variabili ambiente per multi-GPU\nimport os\nos.environ['CUDA_VISIBLE_DEVICES'] = '0,1'  # Usa entrambe le GPU\nos.environ['OLLAMA_NUM_GPU'] = '2'\n\n# Ottimizzazioni NCCL per comunicazioni inter-GPU\nos.environ['NCCL_ALGO'] = 'Ring'\nos.environ['NCCL_NSOCKS_PERTHREAD'] = '4'\nos.environ['NCCL_SOCKET_NTHREADS'] = '2'\nos.environ['OLLAMA_GPU_MEMORY_PIN'] = '1'  # Previene swap memoria [1]\nos.environ['OLLAMA_QUANTIZATION'] = 'q4_1'  # Balance prestazioni-memoria\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:53:19.725525Z","iopub.execute_input":"2025-04-04T07:53:19.725821Z","iopub.status.idle":"2025-04-04T07:53:19.730577Z","shell.execute_reply.started":"2025-04-04T07:53:19.725799Z","shell.execute_reply":"2025-04-04T07:53:19.729748Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# Installazione Python packages\n!pip install -q nvidia-ml-py3 pyngrok ollama --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:53:23.291377Z","iopub.execute_input":"2025-04-04T07:53:23.291692Z","iopub.status.idle":"2025-04-04T07:53:29.991532Z","shell.execute_reply.started":"2025-04-04T07:53:23.291667Z","shell.execute_reply":"2025-04-04T07:53:29.990639Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Download and install ollama to the system\n!curl -fsSL https://ollama.com/install.sh | sh\n\n# Verifica installazione\n!ollama --version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:53:39.591367Z","iopub.execute_input":"2025-04-04T07:53:39.591657Z","iopub.status.idle":"2025-04-04T07:54:26.892544Z","shell.execute_reply.started":"2025-04-04T07:53:39.591636Z","shell.execute_reply":"2025-04-04T07:54:26.891481Z"}},"outputs":[{"name":"stdout","text":">>> Installing ollama to /usr/local\n>>> Downloading Linux amd64 bundle\n############################################################################################# 100.0%###########################                                                             37.0%##########################                                                       43.3%###########################################                                                  48.7%###################################                                         58.8% 74.2%#############################################################################       95.1%########################      95.7%\n>>> Creating ollama user...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\nWarning: could not connect to a running Ollama instance\nWarning: client version is 0.6.4\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import subprocess\nimport threading\nimport logging\n\n# Configurazione logging\nlogging.basicConfig(\n    level=logging.INFO,\n    format='%(asctime)s - %(levelname)s - %(message)s'\n)\nlogger = logging.getLogger(__name__)\n\ndef start_ollama_server():\n    \"\"\"Avvia il server Ollama con supporto multi-GPU\"\"\"\n    cmd = \"ollama serve\"\n    process = subprocess.Popen(\n        cmd.split(),\n        stdout=subprocess.PIPE,\n        stderr=subprocess.STDOUT,\n        text=True\n    )\n    \n    def log_output():\n        \"\"\"Thread per loggare l'output in tempo reale\"\"\"\n        while True:\n            line = process.stdout.readline()\n            if not line and process.poll() is not None:\n                break\n            if line:\n                logger.info(line.strip())\n    \n    thread = threading.Thread(target=log_output, daemon=True)\n    thread.start()\n    \n    print(\"üü¢ Server Ollama avviato con 2 GPU\")\n    return process\n\nserver_process = start_ollama_server()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:54:30.914604Z","iopub.execute_input":"2025-04-04T07:54:30.914973Z","iopub.status.idle":"2025-04-04T07:54:30.925137Z","shell.execute_reply.started":"2025-04-04T07:54:30.914940Z","shell.execute_reply":"2025-04-04T07:54:30.924000Z"}},"outputs":[{"name":"stdout","text":"üü¢ Server Ollama avviato con 2 GPU\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pyngrok import ngrok\nfrom kaggle_secrets import UserSecretsClient\n\ndef setup_ngrok_tunnel(port=11434):\n    \"\"\"Crea un tunnel ngrok sicuro\"\"\"\n    user_secrets = UserSecretsClient()\n    ngrok_token = user_secrets.get_secret(\"NGROK_AUTHTOKEN\")\n    \n    if not ngrok_token:\n        raise ValueError(\"‚ùå Token Ngrok non trovato nei Kaggle Secrets!\")\n    \n    ngrok.set_auth_token(ngrok_token)\n    \n    # Configurazione ottimizzata\n    tunnel = ngrok.connect(\n        port,\n        host_header=f\"localhost:{port}\",\n    )\n    \n    print(f\"üîó Tunnel Ngrok attivo: {tunnel.public_url}\")\n    return tunnel","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:54:42.847266Z","iopub.execute_input":"2025-04-04T07:54:42.847559Z","iopub.status.idle":"2025-04-04T07:54:42.882911Z","shell.execute_reply.started":"2025-04-04T07:54:42.847537Z","shell.execute_reply":"2025-04-04T07:54:42.882317Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"NGROK_PORT = '11434'\nngrok_tunnel = setup_ngrok_tunnel(NGROK_PORT)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:54:48.958405Z","iopub.execute_input":"2025-04-04T07:54:48.958689Z","iopub.status.idle":"2025-04-04T07:54:50.570497Z","shell.execute_reply.started":"2025-04-04T07:54:48.958669Z","shell.execute_reply":"2025-04-04T07:54:50.569618Z"}},"outputs":[{"name":"stdout","text":"üîó Tunnel Ngrok attivo: https://efc7-34-67-153-244.ngrok-free.app                                   \n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import time\nimport sys \n\ndef ollama_run_model(model_name, timeout=300):\n    \"\"\"\n    Esegue un modello Ollama con gestione degli errori\n    \n    Args:\n        model_name (str): Nome del modello (es. 'gemma3:12b')\n        timeout (int): Timeout in secondi\n    \"\"\"\n    \n    start_time = time.time()\n    print(f\"üöÄ Avvio modello '{model_name}'...\")\n    \n    try:\n        cmd = f\"ollama run {model_name}\"\n        with subprocess.Popen(\n            cmd.split(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True\n        ) as proc:\n            \n            # Live output handling\n            while True:\n                line = proc.stdout.readline()\n                if not line and proc.poll() is not None:\n                    break\n                if line:\n                    sys.stdout.write(line)\n                    sys.stdout.flush()\n                \n                # Timeout check\n                if time.time() - start_time > timeout:\n                    raise TimeoutError(f\"Timeout dopo {timeout} secondi\")\n            \n            if proc.returncode != 0:\n                raise RuntimeError(f\"Errore nell'esecuzione (code: {proc.returncode})\")\n    \n    except Exception as e:\n        print(f\"‚ùå Errore: {str(e)}\")\n        raise  \n    finally:\n        print(f\"‚úÖ Tempo esecuzione: {time.time()-start_time:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:54:54.472654Z","iopub.execute_input":"2025-04-04T07:54:54.472986Z","iopub.status.idle":"2025-04-04T07:54:54.479428Z","shell.execute_reply.started":"2025-04-04T07:54:54.472959Z","shell.execute_reply":"2025-04-04T07:54:54.478681Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def ollama_remove_model(model_name, timeout=300):\n    \"\"\"\n    Rimuove un modello Ollama con gestione degli errori\n    \n    Args:\n        model_name (str): Nome del modello (es. 'gemma3:12b')\n        timeout (int): Timeout in secondi\n    \"\"\"\n    \n    start_time = time.time()\n    print(f\"üöÄ Rimozione modello '{model_name}'...\")\n    \n    try:\n        cmd = f\"ollama rm {model_name}\"\n        with subprocess.Popen(\n            cmd.split(),\n            stdout=subprocess.PIPE,\n            stderr=subprocess.STDOUT,\n            text=True\n        ) as proc:\n            \n            # Live output handling\n            while True:\n                line = proc.stdout.readline()\n                if not line and proc.poll() is not None:\n                    break\n                if line:\n                    sys.stdout.write(line)\n                    sys.stdout.flush()\n                \n                # Timeout check\n                if time.time() - start_time > timeout:\n                    raise TimeoutError(f\"Timeout dopo {timeout} secondi\")\n            \n            if proc.returncode != 0:\n                raise RuntimeError(f\"Errore nell'esecuzione (code: {proc.returncode})\")\n    \n    except Exception as e:\n        print(f\"‚ùå Errore: {str(e)}\")\n        raise  \n    finally:\n        print(f\"‚úÖ Tempo esecuzione: {time.time()-start_time:.2f}s\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-04T07:54:57.463342Z","iopub.execute_input":"2025-04-04T07:54:57.463722Z","iopub.status.idle":"2025-04-04T07:54:57.472009Z","shell.execute_reply.started":"2025-04-04T07:54:57.463690Z","shell.execute_reply":"2025-04-04T07:54:57.471018Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Esecuzione modelli\ntry:\n    ollama_run_model(\"gemma3:4b\")\n    #ollama_run_model(\"gemma3:12b\")\n    #ollama_run_model(\"gemma3:27b\")\n    #ollama_run_model(\"deepseek-coder-v2:16b\")\n    #ollama_run_model(\"phi4:14b\")\n    #ollama_run_model(\"phi4-mini:3.8b\")\n    #ollama_run_model(\"llama3.3:70b\")\n\n    ##### DA PROVARE \n    # COGITO 3B-8B-14B \n    #ollama_run_model(\"cogito:8b\") Context: 128K, Per abilitare ragionamento aggiungere al prompt: \"Enable deep thinking subroutine\"\n\n    # DEEPCODER 14B\n    # ollama_run_model(\"deepcoder:14b\") Context: 64K (?)\nexcept KeyboardInterrupt:\n    print(\"Interruzione manuale rilevata\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Rimozione modelli\ntry:\n    ollama_remove_model(\"gemma3:4b\")\n    #ollama_remove_model(\"gemma3:12b\")\n    #ollama_remove_model(\"gemma3:27b\")\n    #ollama_remove_model(\"deepseek-coder-v2:16b\")\n    #ollama_remove_model(\"phi4:14b\")\n    #ollama_remove_model(\"phi4-mini:3.8b\")\n    #ollama_remove_model(\"llama3.3:70b\")\nexcept KeyboardInterrupt:\n    print(\"Interruzione manuale rilevata\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-02T14:05:37.603102Z","iopub.execute_input":"2025-04-02T14:05:37.603432Z","iopub.status.idle":"2025-04-02T14:05:39.616528Z","shell.execute_reply.started":"2025-04-02T14:05:37.603411Z","shell.execute_reply":"2025-04-02T14:05:39.615614Z"}},"outputs":[{"name":"stdout","text":"üöÄ Rimozione modello 'deepseek-coder-v2:16b'...\n\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[K\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hdeleted 'deepseek-coder-v2:16b'\n‚úÖ Tempo esecuzione: 2.01s\n","output_type":"stream"}],"execution_count":11}]}